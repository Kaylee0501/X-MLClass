{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../datasets/AAPD/llm_cluster_result/zero_shot_text_train.jsonl\",\"r\") as json_file:\n",
    "    json_list = list(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list = np.zeros(len(json_list))\n",
    "for i,row in enumerate(json_list):\n",
    "    acc_list[i] = float(json_list[i].split('\"scores\": ')[1][1:].split(', ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_list = np.argsort(acc_list)[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('../../datasets/AAPD/llama_label_50.txt', 'r')\n",
    "document = file1.readlines()[:11037]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for index in rank_list:\n",
    "    documents.append(document[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('../../datasets/AAPD/train_texts_split_50.txt', 'r')\n",
    "docs = file1.readlines()[:11037]\n",
    "original_docs = []\n",
    "for index in rank_list:\n",
    "    original_docs.append(docs[index])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xil240/anaconda3/envs/Multi-Label/lib/python3.9/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sklearn.cluster\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from numpy import savetxt\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\" \n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "\n",
    "cluster_num = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "[[-0.0199602   0.00421679 -0.00083727 ... -0.03149104  0.05162492\n",
      "   0.06221992]\n",
      " [-0.02482701 -0.00586643 -0.01467834 ... -0.05478743  0.00761514\n",
      "   0.03603554]\n",
      " [-0.01608321  0.00441551  0.00027117 ... -0.05026682  0.0122356\n",
      "   0.0833294 ]\n",
      " ...\n",
      " [-0.04761259  0.03474179  0.00177428 ...  0.00370439  0.00936694\n",
      "   0.06089918]\n",
      " [-0.01210953  0.00297768  0.02156829 ... -0.01119911  0.03135183\n",
      "   0.07333915]\n",
      " [-0.05901276 -0.00917071 -0.01007354 ... -0.04394393  0.00919303\n",
      "   0.0564457 ]]\n",
      "(1000, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xil240/anaconda3/envs/Multi-Label/lib/python3.9/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/AAPD/llm_cluster_result/doc_cluster_sec_round.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28mprint\u001b[39m(label)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../datasets/AAPD/llm_cluster_result/doc_cluster_sec_round.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m the_file:\n\u001b[1;32m     55\u001b[0m      \u001b[38;5;28;01mfor\u001b[39;00m content \u001b[38;5;129;01min\u001b[39;00m select_docs_per_label:\n\u001b[1;32m     56\u001b[0m         the_file\u001b[38;5;241m.\u001b[39mwrite(content \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Multi-Label/lib/python3.9/site-packages/IPython/core/interactiveshell.py:308\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/AAPD/llm_cluster_result/doc_cluster_sec_round.txt'"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "instruction = \"Represent documents collected from arxiv abstract for clustering: \"\n",
    "for line in documents:\n",
    "    new_line = line.split(\": \")[1].strip()\n",
    "    docs.append([instruction, new_line])\n",
    "\n",
    "\n",
    "model = INSTRUCTOR('hkunlp/instructor-large')\n",
    "embeddings = model.encode(docs)\n",
    "print(embeddings)\n",
    "print(embeddings.shape)\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "umap_model = UMAP(n_neighbors=5, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "import numpy as np\n",
    "umap_model.fit(embeddings)\n",
    "umap_embeddings = np.nan_to_num(umap_model.transform(embeddings))\n",
    "\n",
    "clustering_model = GaussianMixture(n_components = cluster_num, random_state=0)\n",
    "clustering_model.fit(umap_embeddings)\n",
    "predict_label = clustering_model.predict(umap_embeddings)\n",
    "means = clustering_model.means_\n",
    "\n",
    "# data = {'Document': original_docs, \n",
    "#         'Label': predict_label} \n",
    "# df = pd.DataFrame(data) \n",
    "\n",
    "# df.to_csv(\"../datasets/Reuters-21578/labelSpace/GMM_doc_label_50_test.csv\", index=False)\n",
    "\n",
    "select_docs_per_label = []\n",
    "for label in range(cluster_num):\n",
    "    embedding_list = []\n",
    "    embedding_index = []\n",
    "    cur_mean = means[label]\n",
    "    for i, predict in enumerate(predict_label):\n",
    "        if predict == label:\n",
    "            embedding_list.append(umap_embeddings[i])\n",
    "            embedding_index.append(i)\n",
    "    dis_array = np.zeros(len(embedding_list))\n",
    "    for index, embed in enumerate(embedding_list):\n",
    "        distance = np.linalg.norm(embed - cur_mean)\n",
    "        dis_array[index] = distance\n",
    "    sort_indexs = np.argsort(dis_array)[:3]\n",
    "    doc_list = ''\n",
    "    for cur_index in sort_indexs:\n",
    "        real_index = embedding_index[cur_index]\n",
    "        doc_list += original_docs[real_index].strip() + ' '\n",
    "    select_docs_per_label.append(doc_list)\n",
    "\n",
    "    if label % 50 == 0:\n",
    "        print(label)\n",
    "\n",
    "with open('../datasets/AAPD/llm_cluster_result/doc_cluster_sec_round.txt', 'a') as the_file:\n",
    "     for content in select_docs_per_label:\n",
    "        the_file.write(content + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../datasets/AAPD/llm_cluster_result/doc_cluster_sec_round.txt', 'a') as the_file:\n",
    "     for content in select_docs_per_label:\n",
    "        the_file.write(content + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae39df5f541401cb156ba27b3a1734e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Database Repair and Optimization\n",
      "1: Communication Networks\n",
      "2: Information Retrieval\n",
      "3: Optimization\n",
      "4: Digital Image Forensics\n",
      "5: Social Network Analysis\n",
      "6: Formal Language Theory\n",
      "7: Information Theory\n",
      "8: Communication Networks\n",
      "9: Vehicular Routing and Localization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xil240/anaconda3/envs/Multi-Label/lib/python3.9/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10: Economics\n",
      "11: Signal Processing\n",
      "12: Statistical Modeling\n",
      "13: Data Mining and Clustering\n",
      "14: Communication Networks\n",
      "15: Epidemiology and Public Health\n",
      "16: Information Theory\n",
      "17: Computational Complexity\n",
      "18: Millimeter Wave MIMO Systems\n",
      "19: Data Mining and Machine Learning\n",
      "20: Collaborative Intelligence\n",
      "21: Anomaly Detection in Time Series\n",
      "22: Network Topology\n",
      "23: Machine Learning\n",
      "24: Computational Complexity Theory\n",
      "25: Optimal Power Flow\n",
      "\n",
      "Please let me know if you would like me to label another topic for you.\n",
      "26: Data Analytics\n",
      "27: Simulation and Modeling\n",
      "28: Machine Learning\n",
      "29: Information Theory\n",
      "30: Theoretical Computer Science\n",
      "31: Collaborative Filtering\n",
      "32: Sensor Networks and Localization\n",
      "33: Computational Fluid Dynamics\n",
      "34: Signal Processing\n",
      "35: Neural Networks\n",
      "36: Information Theory\n",
      "37: Information Theory\n",
      "38: Network Analysis\n",
      "39: Wireless Communications\n",
      "40: Educational Psychology\n",
      "41: Communication Systems\n",
      "\n",
      "Please let me know if you would like me to label any other topics or if you have any other questions.\n",
      "42: Computer Vision\n",
      "43: Computer Science: Algorithms and Data Structures\n",
      "44: Mathematics\n",
      "45: Network Performance Optimization\n",
      "46: Data Mining\n",
      "47: User Interface Design and Automated Decision Systems\n",
      "48: Document Analysis\n",
      "49: Information Theory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "file1 = open('../../datasets/AAPD/llm_cluster_result/doc_cluster_sec_round.txt', 'r')\n",
    "documents = file1.readlines()\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant for labeling topics.\n",
    "<</SYS>>\n",
    "\"\"\"\n",
    "\n",
    "#Example prompt demonstrating the output we are looking for\n",
    "example_prompt = \"\"\"\n",
    "I have a topic that contains the following documents:\n",
    "- the relation between pearson 's correlation coefficient and salton 's cosine measure is revealed based on the different possible values of the division of the l1 norm and the l2 norm of a vector \n",
    "- these different values yield a sheaf of increasingly straight lines which form together a cloud of points , being the investigated relation the theoretical results are tested against the author co citation relations \n",
    "- among 24 informetricians for whom two matrices can be constructed , based on co citations the asymmetric occurrence matrix and the symmetric co citation matrix both examples completely confirm the theoretical results \n",
    "- the results enable us to specify an algorithm which provides a threshold value for the cosine above which none of the corresponding pearson correlations would be negative using this threshold value can be expected to optimize the visualization of the vector space\"\n",
    "\n",
    "Based on the information about the topic above, please find one label for this topic above. Make sure you only return the output without anything else in one line.\n",
    "\n",
    "[/INST] Information Retrieval\n",
    "\"\"\"\n",
    "\n",
    "# example_prompt = \"\"\"\n",
    "# I have a topic that contains the following documents:\n",
    "# - OHIO MATTRESS &lt;OMT> MAY HAVE LOWER 1ST QTR NET Ohio Mattress Co said its first quarter, ending February 28, profits may be below the 2.4 mln dlrs, or 15 cts a share, earned in the first quarter of fiscal\n",
    "# - 1986. The company said any decline would be due to expenses related to the acquisitions in the middle of the current quarter of seven licensees of Sealy Inc, as well as 82 pct of\n",
    "# - the outstanding capital stock of Sealy. Because of these acquisitions, it said, first quarter sales will be substantially higher than last year's 67.1 mln dlrs. Noting that it typically reports first quarter results in\n",
    "# - late march, said the report is likely to be issued in early April this year. It said the delay is due to administrative considerations, including conducting appraisals, in connection with the acquisitions.\n",
    "  \n",
    "# Based on the information about the topic above, please find one label for this topic above. Please output your answer use the following format in one line:\n",
    "\n",
    "# [/INST] acquisitions\n",
    "# \"\"\"\n",
    "\n",
    "# example_prompt = \"\"\"\n",
    "# I have a topic that contains the following documents:\n",
    "# - omron hem 790it automatic blood pressure monitor with advanced omron health management software so far this machine has worked well\n",
    "# - and is very simple to use . it is nice to have immediate feedback on the bloodpressure effects of my various exercises , \n",
    "# - food consumption , and relaxation or stress levels .\n",
    "\n",
    "# Based on the information about the topic above, please find one label for this topic above. Please output your answer use the following format in one line:\n",
    "\n",
    "# [/INST] health_personal_care\n",
    "# \"\"\"\n",
    "\n",
    "main_prompt = \"\"\"\n",
    "[INST]\n",
    "I have a topic that contains the following documents:\n",
    "[DOCUMENTS]\n",
    "\n",
    "Based on the information about the topic above, please find one label for this topic above. Make sure you only return the output without anything else in one line.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = system_prompt + example_prompt + main_prompt\n",
    "\n",
    "# Hugging face repo name\n",
    "model = \"meta-llama/Llama-2-13b-chat-hf\" #chat-hf (hugging face wrapper version)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto' # if you have GPU\n",
    ")\n",
    "\n",
    "with open('../../datasets/AAPD/llm_cluster_result/50chunk_labelspace_secround.txt', 'a') as the_file:\n",
    "    for i, doc in enumerate(documents):\n",
    "        replaced_text = prompt.replace('[DOCUMENTS]', doc.strip())\n",
    "        sequences = pipeline(\n",
    "            replaced_text,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            top_p = 0.9,\n",
    "            temperature = 0.2,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        label_text = sequences[0][\"generated_text\"][1250:]\n",
    "        index = label_text.find('[/INST]') + len('[/INST] ')\n",
    "        if label_text[index] == '\\n':\n",
    "            index += 1\n",
    "        the_file.write(str(i) + \": \" + label_text[index:] + '\\n')\n",
    "        print(str(i) + \": \" + label_text[index:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('../../datasets/AAPD/llm_cluster_result/update_labelspace.txt', 'r')\n",
    "documents = file1.readlines()  \n",
    "\n",
    "label_space = []\n",
    "for row in documents:\n",
    "    label = row.strip()\n",
    "    label_space.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Database Repair and Optimization',\n",
       " 'Communication Networks',\n",
       " 'Information Retrieval',\n",
       " 'Optimization',\n",
       " 'Digital Image Forensics',\n",
       " 'Social Network Analysis',\n",
       " 'Formal Language Theory',\n",
       " 'Information Theory',\n",
       " 'Communication Networks',\n",
       " 'Vehicular Routing and Localization',\n",
       " 'Economics',\n",
       " 'Signal Processing',\n",
       " 'Statistical Modeling',\n",
       " 'Data Mining and Clustering',\n",
       " 'Communication Networks',\n",
       " 'Epidemiology and Public Health',\n",
       " 'Information Theory',\n",
       " 'Computational Complexity',\n",
       " 'Millimeter Wave MIMO Systems',\n",
       " 'Data Mining and Machine Learning',\n",
       " 'Collaborative Intelligence',\n",
       " 'Anomaly Detection in Time Series',\n",
       " 'Network Topology',\n",
       " 'Machine Learning',\n",
       " 'Computational Complexity Theory',\n",
       " 'Optimal Power Flow',\n",
       " 'Data Analytics',\n",
       " 'Simulation and Modeling',\n",
       " 'Machine Learning',\n",
       " 'Information Theory',\n",
       " 'Theoretical Computer Science',\n",
       " 'Collaborative Filtering',\n",
       " 'Sensor Networks and Localization',\n",
       " 'Computational Fluid Dynamics',\n",
       " 'Signal Processing',\n",
       " 'Neural Networks',\n",
       " 'Information Theory',\n",
       " 'Information Theory',\n",
       " 'Network Analysis',\n",
       " 'Wireless Communications',\n",
       " 'Educational Psychology',\n",
       " 'Communication Systems',\n",
       " 'Computer Vision',\n",
       " 'Computer Science',\n",
       " 'Mathematics',\n",
       " 'Network Performance Optimization',\n",
       " 'Data Mining',\n",
       " 'User Interface Design and Automated Decision Systems',\n",
       " 'Document Analysis',\n",
       " 'Information Theory']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1 = open('../../datasets/AAPD/llm_cluster_result/50chunk_labelspace_secround.txt', 'r')\n",
    "documents = file1.readlines()\n",
    "new_space = []\n",
    "for row in documents:\n",
    "    new_space.append(row.strip().split(': ')[1])\n",
    "new_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(new_space)):\n",
    "    query_embedding = model.encode(new_space[index])\n",
    "    #print(new_space[index])\n",
    "    passage_embedding = model.encode(label_space)\n",
    "    sim_scores = util.dot_score(query_embedding, passage_embedding)[0].numpy()\n",
    "    rank_list = np.argsort(sim_scores)[-1]\n",
    "    if sim_scores[rank_list] <= 0.45:\n",
    "        label_space.append(new_space[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../datasets/AAPD/llm_cluster_result/update_labelspace1.txt', 'a') as the_file:\n",
    "    for label in label_space:\n",
    "        the_file.write(label + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Multi-Label",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
